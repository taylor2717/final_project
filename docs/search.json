[
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "Exploratory Data Analysis: Diabetes Health Indicators",
    "section": "",
    "text": "# Load required packages\nlibrary(tidyverse)   # data wrangling, plots\nlibrary(janitor)     # clean variable names\nlibrary(skimr)       # data summaries\n\n# Set a reproducible seed\nset.seed(123)"
  },
  {
    "objectID": "EDA.html#introduction",
    "href": "EDA.html#introduction",
    "title": "Exploratory Data Analysis: Diabetes Health Indicators",
    "section": "Introduction",
    "text": "Introduction\nIn this analysis, I explore the Diabetes Health Indicators dataset (diabetes_binary_health_indicators_BRFSS2015.csv), which is based on the Behavioral Risk Factor Surveillance System (BRFSS) survey of adult health behaviors and conditions.\n\nResponse variable: Diabetes_binary\n\n0 = No diabetes\n\n1 = Diabetes\n\n\nI focus on the following predictors (a subset of the full data):\n\nHighBP – high blood pressure\n\nHighChol – high cholesterol\n\nBMI – body mass index (numeric)\n\nSmoker – has smoked at least 100 cigarettes\n\nPhysActivity – physical activity in past 30 days\n\nGenHlth – general health (1 = excellent to 5 = poor)\n\nMentHlth – days of poor mental health\n\nPhysHlth – days of poor physical health\n\nDiffWalk – serious difficulty walking\n\nSex – biological sex\n\nAge – age category\n\nIncome – income category\n\nPurpose of this EDA/Modeling Goals:\n\nDescribe the distributions of key health indicators in this sample.\n\nExplore how these indicators relate to diabetes status (Diabetes_binary).\n\nInform the choice and specification of predictive models that will be built in the accompanying Modeling document, where the ultimate goal is to predict diabetes status from these health indicators."
  },
  {
    "objectID": "EDA.html#data-import-and-cleaning",
    "href": "EDA.html#data-import-and-cleaning",
    "title": "Exploratory Data Analysis: Diabetes Health Indicators",
    "section": "Data Import and Cleaning",
    "text": "Data Import and Cleaning\n\n# Read data from folder\nraw_diabetes &lt;- read_csv(\n\"diabetes_binary_health_indicators_BRFSS2015.csv\"\n)\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Clean column names to be snake_case\ndiabetes &lt;- raw_diabetes %&gt;%\nclean_names()\n\n# Inspect the structure\nglimpse(diabetes)\n\nRows: 253,680\nColumns: 22\n$ diabetes_binary        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,…\n$ high_bp                &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,…\n$ high_chol              &lt;dbl&gt; 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,…\n$ chol_check             &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ bmi                    &lt;dbl&gt; 40, 25, 28, 27, 24, 25, 30, 25, 30, 24, 25, 34,…\n$ smoker                 &lt;dbl&gt; 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,…\n$ stroke                 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,…\n$ heart_diseaseor_attack &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,…\n$ phys_activity          &lt;dbl&gt; 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,…\n$ fruits                 &lt;dbl&gt; 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,…\n$ veggies                &lt;dbl&gt; 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,…\n$ hvy_alcohol_consump    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ any_healthcare         &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ no_docbc_cost          &lt;dbl&gt; 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,…\n$ gen_hlth               &lt;dbl&gt; 5, 3, 5, 2, 2, 2, 3, 3, 5, 2, 3, 3, 3, 4, 4, 2,…\n$ ment_hlth              &lt;dbl&gt; 18, 0, 30, 0, 3, 0, 0, 0, 30, 0, 0, 0, 0, 0, 30…\n$ phys_hlth              &lt;dbl&gt; 15, 0, 30, 0, 0, 2, 14, 0, 30, 0, 0, 30, 15, 0,…\n$ diff_walk              &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,…\n$ sex                    &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,…\n$ age                    &lt;dbl&gt; 9, 7, 9, 11, 11, 10, 9, 11, 9, 8, 13, 10, 7, 11…\n$ education              &lt;dbl&gt; 4, 6, 4, 3, 5, 6, 6, 4, 5, 4, 6, 5, 5, 4, 6, 6,…\n$ income                 &lt;dbl&gt; 3, 1, 8, 6, 4, 8, 7, 4, 1, 3, 8, 1, 7, 6, 2, 8,…\n\n\n\nConverting Variables to Factors\nMany columns are coded as 0/1 or numeric categories, but conceptually are categorical. Here I convert them to factors with meaningful labels where appropriate.\n\ndiabetes &lt;- diabetes %&gt;%\nmutate(\n# Response as factor\ndiabetes_binary = factor(diabetes_binary,\nlevels = c(0, 1),\nlabels = c(\"NoDiabetes\", \"Diabetes\")),\nhigh_bp   = factor(high_bp,   levels = c(0, 1),\n                   labels = c(\"NoHighBP\", \"HighBP\")),\nhigh_chol = factor(high_chol, levels = c(0, 1),\n                   labels = c(\"NoHighChol\", \"HighChol\")),\nsmoker    = factor(smoker,    levels = c(0, 1),\n                   labels = c(\"NonSmoker\", \"Smoker\")),\nphys_activity = factor(phys_activity, levels = c(0, 1),\n                       labels = c(\"NoPA\", \"PhysActive\")),\ndiff_walk = factor(diff_walk, levels = c(0, 1),\n                   labels = c(\"NoDiffWalk\", \"DiffWalk\")),\nsex       = factor(sex,       levels = c(0, 1),\n                   labels = c(\"Female\", \"Male\")),\n\n# General health coded 1–5 (1 = excellent, 5 = poor)\ngen_hlth = factor(gen_hlth,\n                  levels = 1:5,\n                  labels = c(\"Excellent\", \"VeryGood\", \"Good\",\n                             \"Fair\", \"Poor\")),\n\n# Age categories and income categories as ordered factors\nage = factor(age, ordered = TRUE),\nincome = factor(income, ordered = TRUE)\n)\n\n# Quick look after conversions\ndiabetes %&gt;%\nselect(diabetes_binary, high_bp, high_chol, smoker,\nphys_activity, diff_walk, sex, gen_hlth, age, income) %&gt;%\nsummary()\n\n   diabetes_binary       high_bp            high_chol            smoker      \n NoDiabetes:218334   NoHighBP:144851   NoHighChol:146089   NonSmoker:141257  \n Diabetes  : 35346   HighBP  :108829   HighChol  :107591   Smoker   :112423  \n                                                                             \n                                                                             \n                                                                             \n                                                                             \n                                                                             \n    phys_activity         diff_walk          sex              gen_hlth    \n NoPA      : 61760   NoDiffWalk:211005   Female:141974   Excellent:45299  \n PhysActive:191920   DiffWalk  : 42675   Male  :111706   VeryGood :89084  \n                                                         Good     :75646  \n                                                         Fair     :31570  \n                                                         Poor     :12081  \n                                                                          \n                                                                          \n      age            income     \n 9      :33244   8      :90385  \n 10     :32194   7      :43219  \n 8      :30832   6      :36470  \n 7      :26314   5      :25883  \n 11     :23533   4      :20135  \n 6      :19819   3      :15994  \n (Other):87744   (Other):21594  \n\n\n\n\nMissingness\nBefore any analysis, I check for missing values.\n\n# Count missing values per column\n\nmissing_summary &lt;- diabetes %&gt;%\nsummarise(across(everything(), ~ sum(is.na(.)))) %&gt;%\npivot_longer(everything(),\nnames_to = \"variable\",\nvalues_to = \"n_missing\") %&gt;%\narrange(desc(n_missing))\n\nmissing_summary\n\n# A tibble: 22 × 2\n   variable               n_missing\n   &lt;chr&gt;                      &lt;int&gt;\n 1 diabetes_binary                0\n 2 high_bp                        0\n 3 high_chol                      0\n 4 chol_check                     0\n 5 bmi                            0\n 6 smoker                         0\n 7 stroke                         0\n 8 heart_diseaseor_attack         0\n 9 phys_activity                  0\n10 fruits                         0\n# ℹ 12 more rows"
  },
  {
    "objectID": "EDA.html#overall-outcome-balance",
    "href": "EDA.html#overall-outcome-balance",
    "title": "Exploratory Data Analysis: Diabetes Health Indicators",
    "section": "Overall Outcome Balance",
    "text": "Overall Outcome Balance\nFirst, I check how many respondents are labeled with diabetes vs. no diabetes.\n\n# Count how many individuals fall into each diabetes class\ndiabetes %&gt;%\ncount(diabetes_binary) %&gt;%\nmutate(prop = n / sum(n))\n\n# A tibble: 2 × 3\n  diabetes_binary      n  prop\n  &lt;fct&gt;            &lt;int&gt; &lt;dbl&gt;\n1 NoDiabetes      218334 0.861\n2 Diabetes         35346 0.139\n\n\n\nBar Plot of Outcome\n\n# Visualize the distribution of diabetes vs. no-diabetes cases\ndiabetes %&gt;%\nggplot(aes(x = diabetes_binary)) +\ngeom_bar() +\nlabs(\nx = \"Diabetes Status\",\ny = \"Count\",\ntitle = \"Counts of Diabetes vs. No Diabetes\"\n)\n\n\n\n\nDistribution of Diabetes Outcomes\n\n\n\n\nThis plot highlights the strong class imbalance present in the dataset: roughly 86% of respondents report no diabetes, while about 14% report having diabetes. This tells us two important things about the data. First, diabetes is much less common in this BRFSS sample, which is expected but important to quantify. Second, this imbalance will directly affect modeling, because models trained on imbalanced data tend to favor the majority class unless techniques like log-loss or proper tuning are used.\nFrom a data-relationship standpoint, this imbalance also indicates that any predictor-outcome relationships we observe later need to be interpreted with this context—signals associated with diabetes may appear weaker simply because the positive class is much smaller. This reinforces why careful evaluation and appropriate metrics will be important in the modeling stage."
  },
  {
    "objectID": "EDA.html#univariate-summaries-of-key-predictors",
    "href": "EDA.html#univariate-summaries-of-key-predictors",
    "title": "Exploratory Data Analysis: Diabetes Health Indicators",
    "section": "Univariate Summaries of Key Predictors",
    "text": "Univariate Summaries of Key Predictors\nBefore examining how predictors relate to diabetes, it is important to understand each variable on its own. In this section, I explore the distributions of key numeric and categorical predictors to identify typical ranges, levels of variability, and any imbalances in the data. These summaries provide context for interpreting the later bivariate analyses and help highlight variables that may contribute more strongly to distinguishing individuals with and without diabetes. Understanding these one-variable patterns also gives early insight into potential modeling challenges, such as skewed distributions, dominant categories, or low-variability predictors.\n\nNumeric Variables (BMI, Mental & Physical Health Days)\nThe skim summary allows me to examine the distributions of the three numeric variables I am focusing on: BMI, the number of days of poor mental health, and the number of days of poor physical health. All three variables show no missingness, and BMI displays a notably smaller standard deviation compared to the mental and physical health measures, indicating that BMI values are more concentrated while the health-day variables are more spread out. This preliminary look helps identify the natural ranges of these predictors and highlights which ones may contain more variation to explain differences in diabetes status.\n\n# Generate summary statistics for key numeric health indicators\ndiabetes %&gt;%\nselect(bmi, ment_hlth, phys_hlth) %&gt;%\nskim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n253680\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nbmi\n0\n1\n28.38\n6.61\n12\n24\n27\n31\n98\n▇▅▁▁▁\n\n\nment_hlth\n0\n1\n3.18\n7.41\n0\n0\n0\n2\n30\n▇▁▁▁▁\n\n\nphys_hlth\n0\n1\n4.24\n8.72\n0\n0\n0\n3\n30\n▇▁▁▁▁\n\n\n\n\n\n\nBMI Histogram\nBMI is one of the most important clinical indicators of metabolic health, so examining its distribution on its own provides valuable context before comparing it across diabetes groups. Looking at the shape, spread, and skewness of BMI values helps reveal how weight-related differences might contribute to diabetes risk and also signals whether nonlinear modeling techniques may be appropriate. The histogram below shows how BMI is distributed across the entire sample.\n\n# Plot the distribution of BMI values across the entire sample\ndiabetes %&gt;%\nggplot(aes(x = bmi)) +\ngeom_histogram(bins = 30) +\nlabs(\ntitle = \"Distribution of BMI\",\nx = \"BMI\",\ny = \"Count\"\n)\n\n\n\n\nHistogram of BMI\n\n\n\n\nThe histogram of BMI shows a right-skewed distribution, with most individuals falling between a BMI of roughly 20 and 35, but with a long tail extending to very high BMI values above 50. This skewness is clinically reasonable, as extremely high BMIs are less common but meaningful. Because diabetes risk is known to increase with BMI, this distribution suggests that BMI may be an important predictor in our modeling stage. The presence of a long right tail also indicates the potential value of tree-based models, which can naturally capture nonlinear relationships without requiring transformation.\n\n\n\nCategorical Variables\nThe quick frequency tables summarize the distributions of all of the key categorical predictors, including high blood pressure, high cholesterol, smoking status, physical activity, difficulty walking, sex, general health, age category, and income.\n\n# Display frequency counts for major categorical predictors\ndiabetes %&gt;%\nselect(high_bp, high_chol, smoker, phys_activity, diff_walk,\nsex, gen_hlth, age, income) %&gt;%\nmap(~ count(as.data.frame(.), .))  # quick counts\n\n$high_bp\n         .      n\n1 NoHighBP 144851\n2   HighBP 108829\n\n$high_chol\n           .      n\n1 NoHighChol 146089\n2   HighChol 107591\n\n$smoker\n          .      n\n1 NonSmoker 141257\n2    Smoker 112423\n\n$phys_activity\n           .      n\n1       NoPA  61760\n2 PhysActive 191920\n\n$diff_walk\n           .      n\n1 NoDiffWalk 211005\n2   DiffWalk  42675\n\n$sex\n       .      n\n1 Female 141974\n2   Male 111706\n\n$gen_hlth\n          .     n\n1 Excellent 45299\n2  VeryGood 89084\n3      Good 75646\n4      Fair 31570\n5      Poor 12081\n\n$age\n    .     n\n1   1  5700\n2   2  7598\n3   3 11123\n4   4 13823\n5   5 16157\n6   6 19819\n7   7 26314\n8   8 30832\n9   9 33244\n10 10 32194\n11 11 23533\n12 12 15980\n13 13 17363\n\n$income\n  .     n\n1 1  9811\n2 2 11783\n3 3 15994\n4 4 20135\n5 5 25883\n6 6 36470\n7 7 43219\n8 8 90385\n\n\nMost variables show strong imbalance between their categories — for example, far more individuals report NoHighBP than HighBP, and physical activity (PhysActive) is much more common than inactivity (NoPA). These imbalances reflect realistic population patterns, but they also signal that some predictors may carry strong associations with diabetes simply due to the prevalence of certain health behaviors or conditions.\nUnderstanding these distributions is important for two reasons:\n\nModeling impact: Tree-based models and random forests handle categorical imbalance well, but predictors with highly dominant categories may still contribute unevenly.\n\nRelationship context: Categories such as “DiffWalk,” “Poor” general health, or “HighBP” are expected to relate to chronic conditions, including diabetes. Seeing their frequencies now provides context for why they may appear as strong predictors later in the bivariate analysis and model fitting.\n\nTogether, these numeric and categorical summaries provide a foundational understanding of the health indicators in this dataset and show early signs of meaningful differences between individuals with and without diabetes."
  },
  {
    "objectID": "EDA.html#bivariate-relationships-with-diabetes",
    "href": "EDA.html#bivariate-relationships-with-diabetes",
    "title": "Exploratory Data Analysis: Diabetes Health Indicators",
    "section": "Bivariate Relationships With Diabetes",
    "text": "Bivariate Relationships With Diabetes\nIn this section, I explore how some of the key predictors relate directly to the diabetes outcome. While earlier summaries focused on understanding individual variables on their own, the goal here is to examine how differences in health behaviors, conditions, and demographic factors manifest across diabetes groups. These visualizations help identify which predictors show the strongest separation between individuals with and without diabetes and therefore hold the greatest potential predictive value. Understanding these relationships provides a crucial bridge between EDA and the modeling stage, guiding expectations about which variables may drive model performance and how different modeling approaches might capture these patterns.\n\nBMI vs Diabetes Status\n\n# Compare BMI distributions between diabetes and non-diabetes groups\ndiabetes %&gt;%\nggplot(aes(x = diabetes_binary, y = bmi)) +\ngeom_boxplot() +\nlabs(\ntitle = \"BMI by Diabetes Status\",\nx = \"Diabetes Status\",\ny = \"BMI\"\n)\n\n\n\n\nBMI by Diabetes Status\n\n\n\n\nThe boxplot above comparing BMI between individuals with and without diabetes shows a clear difference: those with diabetes tend to have noticeably higher BMI values on average. The median BMI is shifted upward in the diabetes group, and the overall spread is larger, reflecting greater variability among individuals diagnosed with diabetes. This alignment with medical expectations reinforces BMI as an important predictor. For modeling, this suggests a potentially nonlinear relationship — individuals with moderate or high BMI may experience a sharp increase in diabetes risk, which tree-based models can naturally capture.\n\n\nHigh Blood Pressure and Diabetes\n\n# Examine the relationship between high blood pressure and diabetes status\ndiabetes %&gt;%\ncount(diabetes_binary, high_bp) %&gt;%\ngroup_by(diabetes_binary) %&gt;%\nmutate(prop = n / sum(n)) %&gt;%\nggplot(aes(x = diabetes_binary, y = prop, fill = high_bp)) +\ngeom_col(position = \"fill\") +\nscale_y_continuous(labels = scales::percent) +\nlabs(\ntitle = \"High Blood Pressure Status by Diabetes Outcome\",\nx = \"Diabetes Status\",\ny = \"Proportion\",\nfill = \"High BP\"\n)\n\n\n\n\nProportion with High Blood Pressure by Diabetes Status\n\n\n\n\nThe proportional bar chart above indicates that high blood pressure is much more common among individuals with diabetes than among those without. While the majority of the full sample reports no high blood pressure, the share of HighBP cases is substantially higher in the diabetes group. This pattern suggests a strong positive relationship: people with elevated blood pressure are more likely to also have diabetes. Given the strength and clarity of this relationship, high blood pressure may be among the most influential predictors in the modeling phase.\n\n\nGeneral Health and Diabetes\n\n# Visualize how self-reported general health differs by diabetes outcome\ndiabetes %&gt;%\nggplot(aes(x = gen_hlth, fill = diabetes_binary)) +\ngeom_bar(position = \"fill\") +\nscale_y_continuous(labels = scales::percent) +\nlabs(\ntitle = \"General Health and Diabetes Outcome\",\nx = \"Self-reported General Health\",\ny = \"Proportion\",\nfill = \"Diabetes Status\"\n)\n\n\n\n\nGeneral Health vs Diabetes Status\n\n\n\n\nWhen examining general health categories, the distribution shifts dramatically between diabetes groups. Individuals reporting Excellent or VeryGood health are far more prevalent in the non-diabetes group, whereas categories such as Fair and Poor are noticeably over-represented in the diabetes group. This pattern supports the intuitive idea that poorer overall health aligns with chronic conditions like diabetes. It also suggests that the GenHlth variable captures a broad combination of lifestyle and medical risk factors, making it valuable for prediction.\n\n\nPhysical Activity and Diabetes\n\n# Explore how physical activity levels vary across diabetes groups\ndiabetes %&gt;%\nggplot(aes(x = phys_activity, fill = diabetes_binary)) +\ngeom_bar(position = \"fill\") +\nscale_y_continuous(labels = scales::percent) +\nlabs(\ntitle = \"Physical Activity and Diabetes Outcome\",\nx = \"Physical Activity in Past 30 Days\",\ny = \"Proportion\",\nfill = \"Diabetes Status\"\n)\n\n\n\n\nPhysical Activity vs Diabetes Status\n\n\n\n\nThe physical activity comparison reveals that those with diabetes are less likely to report recent physical activity. While physical activity is common in the dataset as a whole, the share of NoPA cases increases among individuals with diabetes. This indicates a meaningful behavioral difference and highlights physical inactivity as a potential risk indicator. This relationship may complement predictors like BMI and blood pressure, offering a lifestyle-based signal in the model."
  },
  {
    "objectID": "EDA.html#summary-of-findings-and-transition-to-modeling",
    "href": "EDA.html#summary-of-findings-and-transition-to-modeling",
    "title": "Exploratory Data Analysis: Diabetes Health Indicators",
    "section": "Summary of Findings and Transition to Modeling",
    "text": "Summary of Findings and Transition to Modeling\nThis exploratory analysis provided a detailed look at how key health indicators differ between individuals with and without diabetes in the BRFSS dataset. Several variables displayed strong and meaningful relationships with diabetes status. In particular, individuals with diabetes tended to have higher BMI values, were more likely to report high blood pressure, were less physically active, and reported poorer overall health. These patterns align with clinical expectations and suggest that multiple lifestyle and health-condition indicators contribute to diabetes risk.\nThe categorical summaries showed substantial imbalance across many predictors, but the direction of those imbalances was informative: categories representing poorer health or more limited mobility were consistently more common among those with diabetes. Meanwhile, variables like age, income, and sex appeared to have weaker but still potentially relevant associations, especially when combined with the stronger health-related predictors.\nOverall, the EDA results highlight which predictors are most promising for building an accurate model and reinforce the need for modeling techniques that can capture nonlinear and interaction-based relationships. With this understanding, the next step is to formally evaluate predictive models using a training/test split and compare performance across a classification tree and a random forest to identify the best approach.\nClick here for the Modeling Page"
  },
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Modeling: Predicting Diabetes from Health Indicators",
    "section": "",
    "text": "In this document, I build and compare predictive models for the Diabetes_binary outcome using the Diabetes Health Indicators dataset. The goal is to develop a model that can estimate the probability that an individual has diabetes based on health indicators such as BMI, blood pressure, cholesterol, physical activity, and general health.\nI focus on two tree-based model families:\n\nClassification Tree – a single decision tree that splits the predictor space using if/else rules.\nRandom Forest – an ensemble of many decision trees fit on bootstrap samples.\n\nThroughout, I use log-loss as the primary performance metric because it evaluates the quality of predicted probabilities, not just hard class labels. Lower log-loss means better calibrated and more accurate probability predictions.\nI first create a train/test split, define a common preprocessing recipe, and then tune each model type using 5-fold cross-validation on the training data. Finally, I compare the tuned models on the test set and select one model to deploy via an API."
  },
  {
    "objectID": "Modeling.html#introduction",
    "href": "Modeling.html#introduction",
    "title": "Modeling: Predicting Diabetes from Health Indicators",
    "section": "",
    "text": "In this document, I build and compare predictive models for the Diabetes_binary outcome using the Diabetes Health Indicators dataset. The goal is to develop a model that can estimate the probability that an individual has diabetes based on health indicators such as BMI, blood pressure, cholesterol, physical activity, and general health.\nI focus on two tree-based model families:\n\nClassification Tree – a single decision tree that splits the predictor space using if/else rules.\nRandom Forest – an ensemble of many decision trees fit on bootstrap samples.\n\nThroughout, I use log-loss as the primary performance metric because it evaluates the quality of predicted probabilities, not just hard class labels. Lower log-loss means better calibrated and more accurate probability predictions.\nI first create a train/test split, define a common preprocessing recipe, and then tune each model type using 5-fold cross-validation on the training data. Finally, I compare the tuned models on the test set and select one model to deploy via an API."
  },
  {
    "objectID": "Modeling.html#traintest-split",
    "href": "Modeling.html#traintest-split",
    "title": "Modeling: Predicting Diabetes from Health Indicators",
    "section": "Train/Test Split",
    "text": "Train/Test Split\nI now split the data into a training set (70%) and a test set (30%). I stratify by the outcome to preserve the overall class imbalance in both sets. The training data will be used for model fitting and tuning, and the test data will be used only once at the end for an honest assessment of performance.\n\n# Split into training and test sets (70/30 split)\n\ndiabetes_split &lt;- initial_split(\ndiabetes,\nprop = 0.7,\nstrata = diabetes_binary\n)\n\ndiabetes_train &lt;- training(diabetes_split)\ndiabetes_test  &lt;- testing(diabetes_split)\n\n# Check class proportions:\n\n# train set\ndiabetes_train %&gt;% count(diabetes_binary) %&gt;%\nmutate(prop = n / sum(n))\n\n# A tibble: 2 × 3\n  diabetes_binary      n  prop\n  &lt;fct&gt;            &lt;int&gt; &lt;dbl&gt;\n1 NoDiabetes      152833 0.861\n2 Diabetes         24742 0.139\n\n\nThese proportions show that approximately the same fraction of “Diabetes” vs. “NoDiabetes” cases is maintained in the training set as in the full data. The strong class imbalance (many more NoDiabetes cases) is preserved, which is important so that the model learns under realistic conditions.\n\n# test set\ndiabetes_test %&gt;% count(diabetes_binary) %&gt;%\nmutate(prop = n / sum(n))\n\n# A tibble: 2 × 3\n  diabetes_binary     n  prop\n  &lt;fct&gt;           &lt;int&gt; &lt;dbl&gt;\n1 NoDiabetes      65501 0.861\n2 Diabetes        10604 0.139\n\n\nThe test set exhibits almost identical class proportions. This confirms that stratified splitting worked correctly and that the test set will provide a fair evaluation of model performance in the presence of class imbalance."
  },
  {
    "objectID": "Modeling.html#common-recipe-for-modeling",
    "href": "Modeling.html#common-recipe-for-modeling",
    "title": "Modeling: Predicting Diabetes from Health Indicators",
    "section": "Common Recipe for Modeling",
    "text": "Common Recipe for Modeling\nTo keep preprocessing consistent across model types, I define one recipe that will be reused for both the classification tree and the random forest. This recipe ensures the outcome is a factor, creates dummy variables for categorical predictors, and drops any predictors with zero variance.\n\n# Define modeling formula\n\nmodel_formula &lt;- diabetes_binary ~ high_bp + high_chol + bmi +\nsmoker + phys_activity + gen_hlth + diff_walk +\nage + income + sex\n\n# Recipe that keeps factors and creates dummy variables\n\ndiabetes_recipe &lt;- recipe(model_formula, data = diabetes_train) %&gt;%\n\n# Ensure outcome is factor\n\nstep_mutate(diabetes_binary = diabetes_binary) %&gt;%\n\n# Create dummy variables for all nominal predictors\n\nstep_dummy(all_nominal_predictors()) %&gt;%\n  \n# Remove predictors with zero variance\n\nstep_zv(all_predictors())\n\ndiabetes_recipe"
  },
  {
    "objectID": "Modeling.html#resampling-setup-and-metric",
    "href": "Modeling.html#resampling-setup-and-metric",
    "title": "Modeling: Predicting Diabetes from Health Indicators",
    "section": "Resampling Setup and Metric",
    "text": "Resampling Setup and Metric\nNext, I set up 5-fold cross-validation on the training data and define log-loss as the primary performance metric.\n\n# Set up 5-fold stratified cross-validation \nset.seed(123)\ndiabetes_folds &lt;- vfold_cv(\ndiabetes_train,\nv = 5,\nstrata = diabetes_binary\n)\n\n# Define the metric set with log loss\n\nlogloss_metric &lt;- metric_set(mn_log_loss)"
  },
  {
    "objectID": "Modeling.html#classification-tree",
    "href": "Modeling.html#classification-tree",
    "title": "Modeling: Predicting Diabetes from Health Indicators",
    "section": "Classification Tree",
    "text": "Classification Tree\n\nWhat is a Classification Tree?\nA classification tree splits the predictor space into regions using a series of if/else rules, such as “Is BMI &gt; 30?” or “Is age category ≥ 8?”. Each terminal node (leaf) corresponds to a region of the predictor space and contains a predicted class probability.\nKey ideas:\n\nEach internal node uses one predictor and one split point to divide the data.\nThe tree grows by recursively splitting nodes to improve class separation.\nTerminal nodes contain predicted class labels / probabilities.\nTrees are interpretable, but a single tree can be unstable and prone to overfitting.\n\nThe main hyperparameter I tune for the tree is the cost_complexity (pruning strength), along with optional control over tree depth.\n\n\nTree Model Specification and Grid\nI now define a classification tree specification with tunable hyperparameters and a grid of values to explore.\n\n# Classification tree model with tunable cost_complexity\ntree_spec &lt;- decision_tree(\ncost_complexity = tune(),   # pruning parameter\ntree_depth      = tune(),\nmin_n           = 20        # minimum observations per node\n) %&gt;%\nset_engine(\"rpart\") %&gt;%\nset_mode(\"classification\")\n\ntree_spec\n\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = tune()\n  tree_depth = tune()\n  min_n = 20\n\nComputational engine: rpart \n\n\nThe model specification shows that cost_complexity and tree_depth will be determined via tuning, while min_n is fixed at 20. Using a larger min_n helps avoid extremely small leaves that might overfit.\nCreate a workflow combining the recipe and model:\n\n# Combine the tree specification with the common recipe into a workflow\ntree_workflow &lt;- workflow() %&gt;%\nadd_model(tree_spec) %&gt;%\nadd_recipe(diabetes_recipe)\n\ntree_workflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_mutate()\n• step_dummy()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = tune()\n  tree_depth = tune()\n  min_n = 20\n\nComputational engine: rpart \n\n\nThis workflow summary confirms that the tree model will always be fit on preprocessed data from the recipe. Keeping the recipe inside the workflow is a best practice in tidymodels and ensures that all resamples receive identical preprocessing.\nNow define a tuning grid for cost_complexity and tree_depth.\n\n# Create a regular grid of cost_complexity and tree_depth values for tuning\ntree_grid &lt;- grid_regular(\ncost_complexity(range = c(-4, -1)),  # on log10 scale\ntree_depth(range = c(3L, 10L)),\nlevels = c(cost_complexity = 5, tree_depth = 4)\n)\n\ntree_grid\n\n# A tibble: 20 × 2\n   cost_complexity tree_depth\n             &lt;dbl&gt;      &lt;int&gt;\n 1        0.0001            3\n 2        0.000562          3\n 3        0.00316           3\n 4        0.0178            3\n 5        0.1               3\n 6        0.0001            5\n 7        0.000562          5\n 8        0.00316           5\n 9        0.0178            5\n10        0.1               5\n11        0.0001            7\n12        0.000562          7\n13        0.00316           7\n14        0.0178            7\n15        0.1               7\n16        0.0001           10\n17        0.000562         10\n18        0.00316          10\n19        0.0178           10\n20        0.1              10\n\n\nThe tuning grid lists all combinations of cost_complexity and tree_depth to be evaluated. Cost complexity spans several orders of magnitude, and tree depths range from shallow (3) to deep (10), allowing the algorithm to explore both simple and complex trees.\n\n\nTune the Tree Using 5-Fold CV (Log-loss)\nUsing 5-fold cross-validation, I evaluate each combination of cost_complexity and tree_depth in the grid.\n\n# Tune the classification tree over the grid using 5-fold CV and log-loss\nset.seed(123)\ntree_res &lt;- tune_grid(\ntree_workflow,\nresamples = diabetes_folds,\ngrid = tree_grid,\nmetrics = logloss_metric,\ncontrol = control_grid(save_pred = TRUE)\n)\n\ntree_res\n\n# Tuning results\n# 5-fold cross-validation using stratification \n# A tibble: 5 × 5\n  splits                 id    .metrics          .notes           .predictions\n  &lt;list&gt;                 &lt;chr&gt; &lt;list&gt;            &lt;list&gt;           &lt;list&gt;      \n1 &lt;split [142059/35516]&gt; Fold1 &lt;tibble [20 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n2 &lt;split [142059/35516]&gt; Fold2 &lt;tibble [20 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n3 &lt;split [142060/35515]&gt; Fold3 &lt;tibble [20 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n4 &lt;split [142061/35514]&gt; Fold4 &lt;tibble [20 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n5 &lt;split [142061/35514]&gt; Fold5 &lt;tibble [20 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n\n\nThis output confirms that 5 stratified folds were successfully created. Because each fold preserves the original class proportions, each resample gives a fair estimate of model performance for an imbalanced classification task.\n\n# Extract cross-validated log-loss results for all hyperparameter combinations\n\n# Collect tuning results\n\ntree_metrics &lt;- tree_res %&gt;% collect_metrics()\n\ntree_metrics\n\n# A tibble: 20 × 8\n   cost_complexity tree_depth .metric     .estimator  mean     n std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n 1        0.0001            3 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 2        0.000562          3 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 3        0.00316           3 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 4        0.0178            3 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 5        0.1               3 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 6        0.0001            5 mn_log_loss binary     0.358     5 5.91e-4 Prepro…\n 7        0.000562          5 mn_log_loss binary     0.358     5 5.91e-4 Prepro…\n 8        0.00316           5 mn_log_loss binary     0.358     5 5.75e-4 Prepro…\n 9        0.0178            5 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n10        0.1               5 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n11        0.0001            7 mn_log_loss binary     0.350     5 3.85e-3 Prepro…\n12        0.000562          7 mn_log_loss binary     0.357     5 6.38e-4 Prepro…\n13        0.00316           7 mn_log_loss binary     0.358     5 6.08e-4 Prepro…\n14        0.0178            7 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n15        0.1               7 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n16        0.0001           10 mn_log_loss binary     0.340     5 2.33e-3 Prepro…\n17        0.000562         10 mn_log_loss binary     0.357     5 9.28e-4 Prepro…\n18        0.00316          10 mn_log_loss binary     0.358     5 6.08e-4 Prepro…\n19        0.0178           10 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n20        0.1              10 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n\n\nThis table reports the mean log-loss for each combination of cost complexity and tree depth across the five folds. Small log-loss values indicate better performance. We can already see that deeper trees (depth 10) with very small cost_complexity values achieve the strongest performance, suggesting that the dataset benefits from models capable of capturing more complex interactions among predictors.\n\n# Plot log-loss vs cost_complexity for a few depths\n\ntree_metrics %&gt;%\nggplot(aes(x = cost_complexity, y = mean,\ncolor = factor(tree_depth))) +\ngeom_line() +\ngeom_point() +\nscale_x_log10() +\nlabs(\ntitle = \"Classification Tree: Log-loss vs Cost Complexity\",\nx = \"Cost Complexity (log10 scale)\",\ny = \"Mean Log-loss (5-fold CV)\",\ncolor = \"Tree Depth\"\n)\n\n\n\n\n\n\n\n\nThe plot visualizes how log-loss changes as we increase cost complexity. For shallow trees, log-loss remains comparatively high regardless of pruning level, indicating underfitting. Deeper trees initially yield lower log-loss when pruning is minimal, but performance worsens as pruning increases. This confirms that the best-performing models are deep trees with very small cost complexity values, aligning with the numeric tuning results above.\n\n\nSelect Best Tree and Evaluate on Test Set\nFrom the tuning results, I select the tree with the lowest cross-validated log-loss. I then finalize the workflow with these hyperparameters and fit the model on the full training set, evaluating it once on the held-out test set. The resulting test-set log-loss provides an unbiased estimate of how well this tuned classification tree calibrates predicted probabilities for new individuals. Because log-loss is on a different scale than accuracy, I will compare this value directly to the random forest’s test log-loss to decide which overall model performs better.\n\n# Identify the hyperparameter combination that minimizes cross-validated log-loss\nbest_tree &lt;- tree_res %&gt;%\nselect_best(metric = \"mn_log_loss\")\n\nbest_tree\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;                \n1          0.0001         10 Preprocessor1_Model16\n\n\nThese results show that the optimal tree uses a very small cost complexity value and the maximum tree depth considered. This configuration allows the tree to capture detailed structure in the data while still using slight pruning to avoid extreme overfitting. This tuned tree will now be evaluated on the held-out test set to estimate its real-world performance.\n\n# Finalize the tree workflow with the best hyperparameters and fit/evaluate on the test set\n\n\n# Finalize workflow with best hyperparameters\n\nfinal_tree_workflow &lt;- tree_workflow %&gt;%\nfinalize_workflow(best_tree)\n\n# Fit to training data and evaluate on test set\n\ntree_final_fit &lt;- last_fit(\nfinal_tree_workflow,\nsplit = diabetes_split,\nmetrics = logloss_metric\n)\n\n# Log-loss on test set\n\ntree_test_metrics &lt;- tree_final_fit %&gt;% collect_metrics()\ntree_test_metrics\n\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 mn_log_loss binary         0.347 Preprocessor1_Model1\n\n\nThe tuned classification tree achieves a test-set log-loss of approximately 0.347. Because log-loss penalizes confident wrong predictions, this value indicates that while the tree captures meaningful patterns, it still produces some overconfident errors—expected given the dataset’s imbalance and the tree’s high flexibility. This result provides a performance baseline before evaluating more powerful ensemble methods."
  },
  {
    "objectID": "Modeling.html#tree-model-specification-and-grid",
    "href": "Modeling.html#tree-model-specification-and-grid",
    "title": "Modeling: Predicting Diabetes from Health Indicators",
    "section": "Tree Model Specification and Grid",
    "text": "Tree Model Specification and Grid\n\n# Classification tree model with tunable cost_complexity\n\ntree_spec &lt;- decision_tree(\ncost_complexity = tune(),   # pruning parameter\ntree_depth      = tune(),   # optional: tune depth as well\nmin_n           = 20        # minimum observations per node\n) %&gt;%\nset_engine(\"rpart\") %&gt;%\nset_mode(\"classification\")\n\ntree_spec\n\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = tune()\n  tree_depth = tune()\n  min_n = 20\n\nComputational engine: rpart \n\n\nCreate a workflow combining the recipe and model:\n\ntree_workflow &lt;- workflow() %&gt;%\nadd_model(tree_spec) %&gt;%\nadd_recipe(diabetes_recipe)\n\ntree_workflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_mutate()\n• step_dummy()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = tune()\n  tree_depth = tune()\n  min_n = 20\n\nComputational engine: rpart \n\n\nNow define a tuning grid for cost_complexity and tree_depth.\n\ntree_grid &lt;- grid_regular(\ncost_complexity(range = c(-4, -1)),  # on log10 scale\ntree_depth(range = c(3L, 10L)),\nlevels = c(cost_complexity = 5, tree_depth = 4)\n)\n\ntree_grid\n\n# A tibble: 20 × 2\n   cost_complexity tree_depth\n             &lt;dbl&gt;      &lt;int&gt;\n 1        0.0001            3\n 2        0.000562          3\n 3        0.00316           3\n 4        0.0178            3\n 5        0.1               3\n 6        0.0001            5\n 7        0.000562          5\n 8        0.00316           5\n 9        0.0178            5\n10        0.1               5\n11        0.0001            7\n12        0.000562          7\n13        0.00316           7\n14        0.0178            7\n15        0.1               7\n16        0.0001           10\n17        0.000562         10\n18        0.00316          10\n19        0.0178           10\n20        0.1              10"
  },
  {
    "objectID": "Modeling.html#tune-the-tree-using-5-fold-cv-log-loss",
    "href": "Modeling.html#tune-the-tree-using-5-fold-cv-log-loss",
    "title": "Modeling: Predicting Diabetes from Health Indicators",
    "section": "Tune the Tree Using 5-Fold CV (Log-loss)",
    "text": "Tune the Tree Using 5-Fold CV (Log-loss)\n\nset.seed(123)\ntree_res &lt;- tune_grid(\ntree_workflow,\nresamples = diabetes_folds,\ngrid = tree_grid,\nmetrics = logloss_metric,\ncontrol = control_grid(save_pred = TRUE)\n)\n\ntree_res\n\n# Tuning results\n# 5-fold cross-validation using stratification \n# A tibble: 5 × 5\n  splits                 id    .metrics          .notes           .predictions\n  &lt;list&gt;                 &lt;chr&gt; &lt;list&gt;            &lt;list&gt;           &lt;list&gt;      \n1 &lt;split [142059/35516]&gt; Fold1 &lt;tibble [20 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n2 &lt;split [142059/35516]&gt; Fold2 &lt;tibble [20 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n3 &lt;split [142060/35515]&gt; Fold3 &lt;tibble [20 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n4 &lt;split [142061/35514]&gt; Fold4 &lt;tibble [20 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n5 &lt;split [142061/35514]&gt; Fold5 &lt;tibble [20 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n\n\n\n# Collect tuning results\n\ntree_metrics &lt;- tree_res %&gt;% collect_metrics()\n\ntree_metrics\n\n# A tibble: 20 × 8\n   cost_complexity tree_depth .metric     .estimator  mean     n std_err .config\n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;  \n 1        0.0001            3 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 2        0.000562          3 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 3        0.00316           3 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 4        0.0178            3 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 5        0.1               3 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n 6        0.0001            5 mn_log_loss binary     0.358     5 5.91e-4 Prepro…\n 7        0.000562          5 mn_log_loss binary     0.358     5 5.91e-4 Prepro…\n 8        0.00316           5 mn_log_loss binary     0.358     5 5.75e-4 Prepro…\n 9        0.0178            5 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n10        0.1               5 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n11        0.0001            7 mn_log_loss binary     0.350     5 3.85e-3 Prepro…\n12        0.000562          7 mn_log_loss binary     0.357     5 6.38e-4 Prepro…\n13        0.00316           7 mn_log_loss binary     0.358     5 6.08e-4 Prepro…\n14        0.0178            7 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n15        0.1               7 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n16        0.0001           10 mn_log_loss binary     0.340     5 2.33e-3 Prepro…\n17        0.000562         10 mn_log_loss binary     0.357     5 9.28e-4 Prepro…\n18        0.00316          10 mn_log_loss binary     0.358     5 6.08e-4 Prepro…\n19        0.0178           10 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n20        0.1              10 mn_log_loss binary     0.404     5 9.73e-6 Prepro…\n\n# Plot log-loss vs cost_complexity for a few depths\n\ntree_metrics %&gt;%\nggplot(aes(x = cost_complexity, y = mean,\ncolor = factor(tree_depth))) +\ngeom_line() +\ngeom_point() +\nscale_x_log10() +\nlabs(\ntitle = \"Classification Tree: Log-loss vs Cost Complexity\",\nx = \"Cost Complexity (log10 scale)\",\ny = \"Mean Log-loss (5-fold CV)\",\ncolor = \"Tree Depth\"\n)\n\n\n\n\nLog-loss Across Tree Tuning Grid"
  },
  {
    "objectID": "Modeling.html#select-best-tree-and-evaluate-on-test-set",
    "href": "Modeling.html#select-best-tree-and-evaluate-on-test-set",
    "title": "Modeling: Predicting Diabetes from Health Indicators",
    "section": "Select Best Tree and Evaluate on Test Set",
    "text": "Select Best Tree and Evaluate on Test Set\n\nbest_tree &lt;- tree_res %&gt;%\nselect_best(metric = \"mn_log_loss\")\n\nbest_tree\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;                \n1          0.0001         10 Preprocessor1_Model16\n\n\n\n# Finalize workflow with best hyperparameters\n\nfinal_tree_workflow &lt;- tree_workflow %&gt;%\nfinalize_workflow(best_tree)\n\n# Fit to training data and evaluate on test set\n\ntree_final_fit &lt;- last_fit(\nfinal_tree_workflow,\nsplit = diabetes_split,\nmetrics = logloss_metric\n)\n\n# Log-loss on test set\n\ntree_test_metrics &lt;- tree_final_fit %&gt;% collect_metrics()\ntree_test_metrics\n\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 mn_log_loss binary         0.347 Preprocessor1_Model1\n\n\nThe classification tree achieves the above log-loss on the test set. Because log-loss penalizes overconfident wrong predictions, a lower value indicates a better calibrated model."
  },
  {
    "objectID": "Modeling.html#random-forest",
    "href": "Modeling.html#random-forest",
    "title": "Modeling: Predicting Diabetes from Health Indicators",
    "section": "Random Forest",
    "text": "Random Forest\n\nWhat is a Random Forest?\nA random forest is an ensemble of many classification trees:\n\nEach tree is trained on a bootstrap sample (sampling with replacement) of the training data.\nAt each split, only a random subset of predictors is considered (mtry).\nPredictions are aggregated (majority vote for classes, or averaging probabilities).\n\nWhy use a random forest?\n\nReduces the variance and instability of a single tree.\nOften much better predictive performance.\nStill somewhat interpretable through variable importance measures.\n\nThe key tuning parameter here is mtry:\n\nSmaller mtry → more randomness between trees, potentially better generalization.\nLarger mtry → trees more similar to each other.\n\nFor this dataset, a random forest can capture complex nonlinear relationships between health indicators and diabetes status by averaging over many different trees. This is especially appealing given the mixture of numeric and categorical predictors and the potential interactions among variables like BMI, high blood pressure, general health, and physical activity. The trade-off is that the random forest is less interpretable than a single tree, so model selection will focus primarily on predictive performance measured via log-loss.\n\n\nRandom Forest Specification and Grid\nI specify a random forest model with a fixed number of trees and a tunable mtry parameter, which controls how many predictors are considered at each split. Smaller mtry values increase randomness between trees and can improve generalization, while larger mtry values make trees more similar to each other. The grid of mtry values explores different levels of predictor subset size so I can see how much randomness is helpful for this problem. As with the tree, I combine this model specification with the same preprocessing recipe to ensure a fair comparison between modeling approaches.\n\n# Specify a tunable random forest model with mtry as the primary hyperparameter\nrf_spec &lt;- rand_forest(\nmtry = tune(), # number of predictors considered at each split\ntrees = 500, # number of trees in the forest\nmin_n = 20 # minimum node size\n) %&gt;%\nset_engine(\"ranger\", importance = \"impurity\") %&gt;% \nset_mode(\"classification\")\n\nrf_spec\n\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  mtry = tune()\n  trees = 500\n  min_n = 20\n\nEngine-Specific Arguments:\n  importance = impurity\n\nComputational engine: ranger \n\n\nThe model specification shows that only mtry will be tuned, while trees and min_n are held fixed. Using 500 trees provides a stable ensemble, and the importance = “impurity” setting allows us to later inspect variable importance if desired.\nWorkflow:\n\n# Build a workflow that pairs the random forest specification with the common recipe\nrf_workflow &lt;- workflow() %&gt;%\nadd_model(rf_spec) %&gt;%\nadd_recipe(diabetes_recipe)\n\nrf_workflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_mutate()\n• step_dummy()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  mtry = tune()\n  trees = 500\n  min_n = 20\n\nEngine-Specific Arguments:\n  importance = impurity\n\nComputational engine: ranger \n\n\nThis workflow summary confirms that the random forest uses the same preprocessing recipe as the classification tree. This alignment ensures that any performance differences reflect the modeling strategy itself.\nGrid for mtry:\n\n# Define a tuning grid over candidate mtry values for the random forest\n\n# Number of predictor columns after dummies\n\nnum_pred &lt;- diabetes_recipe %&gt;%\nprep() %&gt;%\njuice() %&gt;%\nselect(-diabetes_binary) %&gt;%\nncol()\n\n# Create a simple grid for mtry\n\nrf_grid &lt;- tibble(\nmtry = floor(seq(2, num_pred, length.out = 6))\n)\n\nrf_grid\n\n# A tibble: 6 × 1\n   mtry\n  &lt;dbl&gt;\n1     2\n2     7\n3    13\n4    18\n5    24\n6    30\n\n\nThe value of num_pred indicates how many predictor columns exist after dummy variables are created. I use this number to construct a sensible range of candidate mtry values.\nThe mtry grid explores six mtry values from very small to relatively large. Smaller mtry values force more randomness between trees, whereas larger values make trees more similar. The goal is to find the mtry that yields the lowest log-loss.\n\n\nTune the Random Forest with 5-Fold CV (Log-loss)\nI repeat the same 5-fold cross-validation procedure for the random forest, evaluating log-loss for each candidate mtry value. The results indicate which level of predictor subsampling strikes the best balance between bias and variance. In this dataset, the best-performing mtry value corresponds to a model that allows enough randomness to diversify the trees while still considering a reasonable number of predictors at each split. Compared to the single tree, the random forest tends to achieve lower log-loss, reflecting its ability to average over many decision boundaries and reduce overfitting.\n\n# Tune the random forest over the mtry grid using 5-fold CV and log-loss\nset.seed(123)\nrf_res &lt;- tune_grid(\nrf_workflow,\nresamples = diabetes_folds,\ngrid = rf_grid,\nmetrics = logloss_metric,\ncontrol = control_grid(save_pred = TRUE)\n)\n\nrf_res\n\n# Tuning results\n# 5-fold cross-validation using stratification \n# A tibble: 5 × 5\n  splits                 id    .metrics         .notes           .predictions\n  &lt;list&gt;                 &lt;chr&gt; &lt;list&gt;           &lt;list&gt;           &lt;list&gt;      \n1 &lt;split [142059/35516]&gt; Fold1 &lt;tibble [6 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n2 &lt;split [142059/35516]&gt; Fold2 &lt;tibble [6 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n3 &lt;split [142060/35515]&gt; Fold3 &lt;tibble [6 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n4 &lt;split [142061/35514]&gt; Fold4 &lt;tibble [6 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n5 &lt;split [142061/35514]&gt; Fold5 &lt;tibble [6 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n\n\nThe tuning results confirm that each mtry value was evaluated using 5-fold stratified cross-validation, giving a reliable estimate of performance for each configuration.\n\n# Summarize cross-validated log-loss for each candidate mtry value\nrf_metrics &lt;- rf_res %&gt;% collect_metrics()\n\nrf_metrics\n\n# A tibble: 6 × 7\n   mtry .metric     .estimator  mean     n  std_err .config             \n  &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1     2 mn_log_loss binary     0.327     5 0.000455 Preprocessor1_Model1\n2     7 mn_log_loss binary     0.325     5 0.000665 Preprocessor1_Model2\n3    13 mn_log_loss binary     0.333     5 0.00110  Preprocessor1_Model3\n4    18 mn_log_loss binary     0.339     5 0.00158  Preprocessor1_Model4\n5    24 mn_log_loss binary     0.347     5 0.00252  Preprocessor1_Model5\n6    30 mn_log_loss binary     0.361     5 0.00245  Preprocessor1_Model6\n\n\nThe metrics table shows mean log-loss for each mtry value. The smallest log-loss occurs near mtry = 7, with performance worsening as mtry increases. This suggests that moderate randomness in predictor selection leads to the best generalization.\n\n# Plot mean log-loss against mtry to visualize the best-performing random forest configuration\nrf_metrics %&gt;%\nggplot(aes(x = mtry, y = mean)) +\ngeom_line() +\ngeom_point() +\nlabs(\ntitle = \"Random Forest: Log-loss vs mtry\",\nx = \"mtry\",\ny = \"Mean Log-loss (5-fold CV)\"\n)\n\n\n\n\n\n\n\n\nThe plot reinforces the table: the lowest mean log-loss is achieved when mtry is small. As mtry increases, log-loss rises, showing that using too many predictors at each split reduces the benefits of randomization.\n\n\nSelect Best Random Forest and Evaluate on Test Set\nAs with the classification tree, I select the random forest configuration with the lowest cross-validated log-loss and refit it on the full training set. Evaluating this finalized model on the test set yields a test log-loss that can be compared directly to the tuned tree’s test log-loss. Because both models use the same train/test split, recipe, and evaluation metric, any difference in performance can be attributed to the modeling approach itself rather than to differences in data processing.\n\n# Choose the random forest hyperparameters that minimize cross-validated log-loss\nbest_rf &lt;- rf_res %&gt;%\nselect_best(metric = \"mn_log_loss\")\n\nbest_rf\n\n# A tibble: 1 × 2\n   mtry .config             \n  &lt;dbl&gt; &lt;chr&gt;               \n1     7 Preprocessor1_Model2\n\n\nThe selected mtry value (here, mtry = 7) is the configuration that delivered the lowest cross-validated log-loss. This choice strikes a good balance between tree diversity and predictive strength.\n\n# Finalize the random forest workflow with the best mtry and evaluate it on the test set\nfinal_rf_workflow &lt;- rf_workflow %&gt;%\nfinalize_workflow(best_rf)\n\nrf_final_fit &lt;- last_fit(\nfinal_rf_workflow,\nsplit = diabetes_split,\nmetrics = logloss_metric\n)\n\n# Compute log-loss on the held-out test data for the tuned random forest\nrf_test_metrics &lt;- rf_final_fit %&gt;% collect_metrics()\nrf_test_metrics\n\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 mn_log_loss binary         0.325 Preprocessor1_Model1\n\n\nOn the held-out test set, the tuned random forest achieves the log-loss reported in the table (around 0.325). This is lower than the classification tree’s log-loss, indicating that the forest provides better calibrated probability predictions for diabetes status."
  },
  {
    "objectID": "Modeling.html#final-model-selection",
    "href": "Modeling.html#final-model-selection",
    "title": "Modeling: Predicting Diabetes from Health Indicators",
    "section": "Final Model Selection",
    "text": "Final Model Selection\nNow that both model families have been tuned and evaluated on the test set, I place their results side-by-side to choose the overall winner.\n\n# Compare test-set log-loss for the tuned tree and tuned random forest to choose the final model\n\n# Combine test metrics for comparison\nmodel_compare &lt;- bind_rows(\ntree_test_metrics %&gt;% mutate(model = \"Classification Tree\"),\nrf_test_metrics   %&gt;% mutate(model = \"Random Forest\")\n) %&gt;%\nrelocate(model)\n\nmodel_compare\n\n# A tibble: 2 × 5\n  model               .metric     .estimator .estimate .config             \n  &lt;chr&gt;               &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 Classification Tree mn_log_loss binary         0.347 Preprocessor1_Model1\n2 Random Forest       mn_log_loss binary         0.325 Preprocessor1_Model1\n\n\nThis comparison table clearly shows that the random forest has lower test-set log-loss than the classification tree. Because both models were trained and evaluated using the same data splits and metric, the difference reflects a genuine performance advantage. I therefore select the random forest as the final model to deploy."
  },
  {
    "objectID": "Modeling.html#fit-final-random-forest-to-the-full-data",
    "href": "Modeling.html#fit-final-random-forest-to-the-full-data",
    "title": "Modeling: Predicting Diabetes from Health Indicators",
    "section": "Fit Final Random Forest to the Full Data",
    "text": "Fit Final Random Forest to the Full Data\nAfter selecting the random forest as the best model, I refit it on the full dataset (training plus test data) using the chosen hyperparameters. This final fit uses all available information to learn the relationship between predictors and diabetes status, which is important for deployment: the API will use this version of the model to generate predictions for new individuals. Because the model choice and evaluation were already made using a separate test set, refitting on the full data does not bias the earlier comparison but does maximize the data used for the final model.\n\n# Refit the chosen random forest on the full dataset to create the deployment model\n\n# Reuse the recipe, now prepping it on the full data\n\nfinal_recipe_full &lt;- diabetes_recipe %&gt;%\nprep(training = diabetes)\n\nfinal_rf_fit_full &lt;- final_rf_workflow %&gt;%\nfit(data = diabetes)\n\nfinal_rf_fit_full\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_mutate()\n• step_dummy()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~7,      x), num.trees = ~500, min.node.size = min_rows(~20, x), importance = ~\"impurity\",      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1), probability = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  500 \nSample size:                      253680 \nNumber of independent variables:  30 \nMtry:                             7 \nTarget node size:                 20 \nVariable importance mode:         impurity \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.09975817 \n\n\nThe printed workflow confirms that the final random forest (500 trees, mtry = 7) has been trained on the full dataset. Training on all observations allows the final model to capture as much information as possible while still benefiting from the prior model selection process carried out on a separate test set."
  },
  {
    "objectID": "Modeling.html#confusion-matrix-on-test-set",
    "href": "Modeling.html#confusion-matrix-on-test-set",
    "title": "Modeling: Predicting Diabetes from Health Indicators",
    "section": "Confusion Matrix on Test Set",
    "text": "Confusion Matrix on Test Set\nAlthough log-loss is the primary performance metric, it is also useful to examine a confusion matrix for the test set using a 0.5 probability cutoff. This provides insight into how often the model correctly identifies individuals with and without diabetes.\n\n# Collect predictions on test set from last_fit\n\nrf_test_preds &lt;- rf_final_fit %&gt;%\ncollect_predictions()\n\n# Hard classification using 0.5 cutoff on Diabetes probability\n\nrf_test_preds &lt;- rf_test_preds %&gt;%\nmutate(\n.pred_class = if_else(.pred_Diabetes &gt;= 0.5,\n\"Diabetes\", \"NoDiabetes\") %&gt;%\nfactor(levels = levels(diabetes$diabetes_binary))\n)\n\n# Confusion matrix\n\nrf_conf &lt;- rf_test_preds %&gt;%\nconf_mat(truth = diabetes_binary, estimate = .pred_class)\n\nrf_conf\n\n            Truth\nPrediction   NoDiabetes Diabetes\n  NoDiabetes      64094     8978\n  Diabetes         1407     1626\n\nautoplot(rf_conf, type = \"heatmap\") +\nscale_fill_gradient() +\nlabs(\ntitle = \"Confusion Matrix: Final Random Forest (Test Set)\"\n)\n\n\n\n\nConfusion Matrix for Final Random Forest on Test Set\n\n\n\n\nNumerically, the confusion matrix shows that the model correctly classifies most NoDiabetes cases (true negatives) while also identifying a meaningful number of Diabetes cases (true positives). However, there are still both false negatives (missed diabetes cases) and false positives. This reflects the challenge posed by the substantial class imbalance in the data.\nThe heatmap visually highlights the same pattern: a large block of correctly predicted non-diabetes cases and a smaller but still visible block of correctly predicted diabetes cases. The off-diagonal cells represent misclassifications, reminding us that even though the random forest performs best according to log-loss, it is not perfect and should be interpreted with care in practice."
  },
  {
    "objectID": "Modeling.html#conclusion",
    "href": "Modeling.html#conclusion",
    "title": "Modeling: Predicting Diabetes from Health Indicators",
    "section": "Conclusion",
    "text": "Conclusion\n\nI split the data into a 70/30 train/test split, preserving the strong class imbalance via stratification.\nI used a common recipe to preprocess predictors (dummy variables, zero-variance filtering) for both models.\nI tuned:\n\nA classification tree over cost_complexity and tree_depth.\nA random forest over mtry.\n\nBoth models were tuned using 5-fold cross-validation with log-loss as the primary metric.\nOn the held-out test set, the random forest achieved a lower log-loss than the classification tree, indicating better calibrated probability predictions for diabetes risk.\nI refit the final random forest on the full dataset to create a deployment-ready model and evaluated its classification behavior using a confusion matrix.\n\nIn the next step of the project, this final random forest model is used in a plumber API and served through a Docker container, allowing external users to submit new patient profiles and obtain estimated probabilities of diabetes."
  },
  {
    "objectID": "Modeling.html#load-packages-and-data",
    "href": "Modeling.html#load-packages-and-data",
    "title": "Modeling: Predicting Diabetes from Health Indicators",
    "section": "Load Packages and Data",
    "text": "Load Packages and Data\nI start by loading the packages needed for modeling and importing the dataset. The same data used in the EDA document is re-used here so that modeling builds directly on earlier exploration.\n\n# Load required packages\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(tidymodels)  # for modeling framework\n\nset.seed(123)\n\n# Set tidymodels to silence messages\ntidymodels_prefer()\n\n# Import and clean data (same steps as EDA to keep consistent)\ndiabetes &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\") %&gt;%\n  clean_names() %&gt;%\n  mutate(\n    diabetes_binary = factor(diabetes_binary,\n                             levels = c(0, 1),\n                             labels = c(\"NoDiabetes\", \"Diabetes\")),\n    high_bp   = factor(high_bp,   levels = c(0, 1),\n                       labels = c(\"NoHighBP\", \"HighBP\")),\n    high_chol = factor(high_chol, levels = c(0, 1),\n                       labels = c(\"NoHighChol\", \"HighChol\")),\n    smoker    = factor(smoker,    levels = c(0, 1),\n                       labels = c(\"NonSmoker\", \"Smoker\")),\n    phys_activity = factor(phys_activity, levels = c(0, 1),\n                           labels = c(\"NoPA\", \"PhysActive\")),\n    diff_walk = factor(diff_walk, levels = c(0, 1),\n                       labels = c(\"NoDiffWalk\", \"DiffWalk\")),\n    sex       = factor(sex,       levels = c(0, 1),\n                       labels = c(\"Female\", \"Male\")),\n    gen_hlth = factor(gen_hlth,\n                      levels = 1:5,\n                      labels = c(\"Excellent\", \"VeryGood\", \"Good\",\n                                 \"Fair\", \"Poor\")),\n    age    = factor(age,    ordered = TRUE),\n    income = factor(income, ordered = TRUE)\n  )\n\nThe code above reads in the data from the project folder, standardizes column names, and recodes the Diabetes_binary variable into a factor with labels “NoDiabetes” and “Diabetes.” This prepares the outcome for classification modeling."
  }
]